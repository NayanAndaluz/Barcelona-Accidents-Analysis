{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accident Analysis in Barcelona\n",
    "In this notebook, I’ll be working with accident data from the Urban Guard in Barcelona. I’ll clean the data, explore patterns through EDA, and build a modeling pipeline to understand what factors contribute to accidents.This will help uncover insights that could help improve road safety in the city\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    precision_score,\n",
    "    r2_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = \"./data/2023_accidents_causa_conductor_gu_bcn_.csv\"\n",
    "data = pd.read_csv(csv_dir)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num_postal has an error in the colum name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=data.columns.str.strip() #Num_postal with typo in the column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will clean any typos in the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_columns(df):\n",
    "    return df.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols=get_categorical_columns(data)\n",
    "\n",
    "for col in cat_cols:\n",
    "    if data[col].isnull().any():\n",
    "        data[col] = data[col].fillna('')\n",
    "    data[col] = data[col].str.strip()\n",
    "    data[col] = data[col].str.replace(' ', '_')\n",
    "    \n",
    "print(f\"Cleaned categorical columns:\\n{'\\n'.join(cat_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nom_districte has 104 rows that are 'desconegut'(unknown), I will drop them\n",
    "\n",
    "data=data[data['Nom_districte']!='desconegut']\n",
    "\n",
    "count_desconegut = (data['Nom_districte'] == 'desconegut').sum()\n",
    "print(count_desconegut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(f'{col}: {data[col].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codi_carrer cleaning    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codi_carrer unique values: 1040. nom_carrer: unique values: 1073\n",
    "data.drop('Codi_carrer', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicated rows:{data.duplicated().sum()}')\n",
    "print (f'first row duplicates: {data['Numero_expedient'].duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some accidents have the same 'Numero_expedient' but different 'Descripcio_causa_mediata',\n",
    "# so I'm keeping the first one and dropping the rest\n",
    "\n",
    "duplicates = data.duplicated(subset=['Numero_expedient'], keep=False)\n",
    "\n",
    "# Filtering rows that are not duplicates or that do not have 'Altres' in 'Descripcio_causa_mediata'\n",
    "\n",
    "data = data.loc[~((data['Descripcio_causa_mediata'] == 'altres') & duplicates)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 20))  # Adjusted nrows and ncols\n",
    "\n",
    "# Plot 1: Histogram of accidents by hour of the day\n",
    "data.groupby([\"Hora_dia\"])['Numero_expedient'].count().plot(kind=\"bar\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Number of Accidents by Hour of the Day')\n",
    "axes[0, 0].set_xlabel('Hour of the Day')\n",
    "axes[0, 0].set_ylabel('Number of Accidents')\n",
    "\n",
    "# Plot 2: Number of accidents by district\n",
    "data.groupby([\"Nom_districte\"])['Numero_expedient'].count().sort_values(ascending=False).plot(kind=\"bar\", ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Number of Accidents by District')\n",
    "axes[0, 1].set_xlabel('District')  # Changed 'Day' to 'District'\n",
    "axes[0, 1].set_ylabel('Number of Accidents')\n",
    "\n",
    "# Plot 3: Number of accidents by cause\n",
    "data.groupby([\"Descripcio_causa_mediata\"])['Numero_expedient'].count().sort_values(ascending=False).plot(kind=\"bar\", ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Number of Accidents by Cause')\n",
    "axes[1, 0].set_xlabel('Cause')\n",
    "axes[1, 0].set_ylabel('Number of Accidents')\n",
    "\n",
    "# Plot 4: Number of accidents by parte of the day\n",
    "data.groupby([\"Descripcio_torn\"])['Numero_expedient'].count().sort_values(ascending=False).plot(kind=\"bar\", ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Number of Accidents by Part of the Day')\n",
    "axes[1, 1].set_xlabel('Part of the Day')\n",
    "axes[1, 1].set_ylabel('Number of Accidents')\n",
    "\n",
    "# Plot 5: Number of accidents by day of the week (Chronological Order)\n",
    "data.groupby([\"Descripcio_dia_setmana\"])['Numero_expedient'].count().plot(kind=\"bar\", ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Number of Accidents by Day of the Week')\n",
    "axes[2, 0].set_xlabel('Day')\n",
    "axes[2, 0].set_ylabel('Number of Accidents')\n",
    "\n",
    "# Plot 6: Number of accidents by day of the month (Chronological Order)\n",
    "day_month_order = range(1, 32)  # Days in a month\n",
    "data['Dia_mes'] = pd.Categorical(data['Dia_mes'], categories=day_month_order, ordered=True)\n",
    "data.groupby([\"Dia_mes\"])['Numero_expedient'].count().plot(kind=\"bar\", ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Number of Accidents by Day of the Month')\n",
    "axes[2, 1].set_xlabel('Day of the Month')\n",
    "axes[2, 1].set_ylabel('Number of Accidents')\n",
    "\n",
    "# Plot 7: Number of accidents by month (Chronological Order)\n",
    "data.groupby([\"Mes_any\"])['Numero_expedient'].count().plot(kind=\"bar\", ax=axes[3, 0])\n",
    "axes[3, 0].set_title('Number of Accidents by Month')\n",
    "axes[3, 0].set_xlabel('Month')\n",
    "axes[3, 0].set_ylabel('Number of Accidents')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display all plots together\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid or missing latitude/longitude values\n",
    "data = data.dropna(subset=['Latitud_WGS84', 'Longitud_WGS84'])\n",
    "data = data[(data['Latitud_WGS84'].notnull()) & (data['Longitud_WGS84'].notnull())]\n",
    "\n",
    "# Convert to list of lists\n",
    "heat_data = [[float(row['Latitud_WGS84']), float(row['Longitud_WGS84'])] for index, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folium map \n",
    "barcelona_map = folium.Map(location=[41.41, 2.16], zoom_start=12.5)\n",
    "\n",
    "# Add the heatmap layer\n",
    "HeatMap(heat_data, blur=30, min_opacity=0.2).add_to(barcelona_map)\n",
    "barcelona_map.save(\"barcelona_heatmap.html\")\n",
    "\n",
    "def render_folium_map(map_object, html_path, png_path, width=1000, height=800, wait_time=5):\n",
    "    \"\"\"\n",
    "    Render a Folium map as a static image and save both HTML and PNG versions.\n",
    "    \n",
    "    Parameters:\n",
    "    - map_object: A Folium map object\n",
    "    - html_path: Path to save the HTML version of the map\n",
    "    - png_path: Path to save the PNG version of the map\n",
    "    - width: Width of the browser window (default 1000)\n",
    "    - height: Height of the browser window (default 800)\n",
    "    - wait_time: Time to wait for the map to render in seconds (default 5)\n",
    "    \n",
    "    Returns:\n",
    "    - Displays the PNG image in the notebook\n",
    "    \"\"\"\n",
    "    # change routes to path for compability\n",
    "    html_path = Path(html_path).resolve()\n",
    "    png_path = Path(png_path).resolve()\n",
    "    \n",
    "    # Save the map to the specified HTML file\n",
    "    map_object.save(html_path)\n",
    "    \n",
    "    # Set up the web driver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # Run in background\n",
    "    options.add_argument('--disable-gpu')  # Disable GPU for headless mode\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    try:\n",
    "        # Open the HTML file\n",
    "        driver.get(f\"file://{html_path}\")\n",
    "        \n",
    "        # Wait for the map to render\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # Set the size of the browser window\n",
    "        driver.set_window_size(width, height)\n",
    "        \n",
    "        # Capture the screenshot\n",
    "        driver.save_screenshot(png_path)\n",
    "        \n",
    "        # Display the image in the notebook\n",
    "        \n",
    "        display(Image.open(png_path))\n",
    "\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "# Call the function\n",
    "render_folium_map(barcelona_map, \"data/barcelona_heatmap.html\", \"data/barcelona_heatmap.png\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Insight Analysis\n",
    "\n",
    "We can see that **Eixample** is the district with the most accidents, and within it, the neighborhood **La Dreta de Eixample** leads the chart.\n",
    "\n",
    "Additionally, there is a noticeable trend of an increase in the number of accidents during specific hours, mainly between **2:00 PM and 8:00 PM**.\n",
    "\n",
    "Regarding the cause, **lack of attention while driving** is the leading cause of accidents.\n",
    "\n",
    "There doesn’t seem to be a significant difference in the number of accidents by day of the week, except for a decrease on **Saturdays and Sundays**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation for Choosing a Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to approach this problem as a classification task rather than a time series prediction because the target variable represents \n",
    "\n",
    "categorical accident. These do not follow a clear temporal evolution but instead depend on multiple static and contextual factors, such as \n",
    "\n",
    "location, day of the week, time of day, and other categorical and numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_time_features(df):\n",
    "    \"\"\"\n",
    "    Generates time-based features from the dataset.\n",
    "    \n",
    "    Features created:\n",
    "    - Cyclic encoding for the hour of the day using sine and cosine transformations.\n",
    "    - Binary feature indicating whether the day is a weekend.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing a 'Hora_dia' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added time-based features.\n",
    "    \"\"\"\n",
    "    df['hora_sin'] = np.sin(2 * np.pi * df['Hora_dia'] / 24)\n",
    "    df['hora_cos'] = np.cos(2 * np.pi * df['Hora_dia'] / 24)\n",
    "    df['es_fin_semana'] = df['Descripcio_dia_setmana'].isin(['Dissabte', 'Diumenge']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepares features for model training by encoding categorical variables and scaling numerical features.\n",
    "    \n",
    "    Steps:\n",
    "    1. Adds time-based features.\n",
    "    2. Defines categorical and numerical feature lists.\n",
    "    3. Encodes categorical features using Label Encoding.\n",
    "    4. Scales numerical features using StandardScaler.\n",
    "    5. Encodes target variable using Label Encoding.\n",
    "    6. Prints the class distribution.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing raw features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - X_encoded (pd.DataFrame): Processed feature set.\n",
    "            - y (np.array): Encoded target variable.\n",
    "            - categorical_features (list): List of categorical feature names.\n",
    "            - le_y (LabelEncoder): LabelEncoder fitted on the target variable.\n",
    "    \"\"\"\n",
    "    df = create_time_features(df)\n",
    "\n",
    "    categorical_features = ['Nom_districte', 'Nom_barri', 'Descripcio_dia_setmana', 'Nom_mes', 'Descripcio_torn']\n",
    "    numeric_features = ['Hora_dia', 'Dia_mes', 'Mes_any', 'Coordenada_UTM_X_ED50', 'Coordenada_UTM_Y_ED50', \n",
    "                        'hora_sin', 'hora_cos', 'es_fin_semana']\n",
    "\n",
    "    X = df[categorical_features + numeric_features].copy()\n",
    "    y = df['Descripcio_causa_mediata'].copy()\n",
    "\n",
    "    # Encoding categorical features\n",
    "    X_encoded = X.copy()\n",
    "    for cat_col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[cat_col] = le.fit_transform(X[cat_col])\n",
    "\n",
    "    # Scaling numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_encoded[numeric_features] = scaler.fit_transform(X_encoded[numeric_features])\n",
    "\n",
    "    # Encoding target variable\n",
    "    le_y = LabelEncoder()\n",
    "    y = le_y.fit_transform(y)\n",
    "\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(pd.Series(y).value_counts().sort_index())\n",
    "\n",
    "    return X_encoded, y, categorical_features, le_y\n",
    "\n",
    "def objective(trial, X, y, categorical_features):\n",
    "    \"\"\"\n",
    "    Defines the objective function for hyperparameter optimization using Optuna.\n",
    "    \n",
    "    Steps:\n",
    "    1. Suggests values for LightGBM hyperparameters.\n",
    "    2. Uses StratifiedKFold for cross-validation.\n",
    "    3. Balances the training data using SMOTE.\n",
    "    4. Trains a LightGBM model and evaluates using ROC AUC.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object.\n",
    "        X (pd.DataFrame): Training features.\n",
    "        y (np.array): Target variable.\n",
    "        categorical_features (list): List of categorical feature names.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean ROC AUC score across validation folds.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_class': len(np.unique(y)),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'is_unbalance': True\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(\n",
    "            X_train_res, y_train_res,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=20)]\n",
    "        )\n",
    "\n",
    "        pred = model.predict_proba(X_val)\n",
    "        score = roc_auc_score(y_val, pred, multi_class='ovr')\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def train_and_evaluate_models_fast(X, y, categorical_features):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a LightGBM model using Optuna for hyperparameter tuning.\n",
    "    \n",
    "    Steps:\n",
    "    1. Splits data into training and test sets (stratified).\n",
    "    2. Applies SMOTE-Tomek to balance training data.\n",
    "    3. Optimizes hyperparameters with Optuna.\n",
    "    4. Trains final model using best hyperparameters.\n",
    "    5. Evaluates model performance and returns metrics.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature set.\n",
    "        y (np.array): Target variable.\n",
    "        categorical_features (list): List of categorical features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: \n",
    "            - results (dict): Model, classification report, confusion matrix, feature importance, ROC AUC.\n",
    "            - X_train, X_test, y_train, y_test (splitted data).\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"\\nApplying SMOTE-Tomek for class balancing...\")\n",
    "    smt = SMOTETomek(random_state=42)\n",
    "    X_train_res, y_train_res = smt.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nOptimizing hyperparameters...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train_res, y_train_res, categorical_features), n_trials=20)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"\\nBest hyperparameters:\", best_params)\n",
    "\n",
    "    final_model = lgb.LGBMClassifier(\n",
    "        **best_params, random_state=42, verbose=-1, objective='multiclass', num_class=len(np.unique(y))\n",
    "    )\n",
    "\n",
    "    final_model.fit(X_train_res, y_train_res, eval_set=[(X_test, y_test)], \n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=20)])\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    y_pred_proba = final_model.predict_proba(X_test)\n",
    "\n",
    "    results = {\n",
    "        'model': final_model,\n",
    "        'classification_report': classification_report(y_test, y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'feature_importance': pd.DataFrame({'feature': X.columns, 'importance': final_model.feature_importances_})\n",
    "                              .sort_values('importance', ascending=False),\n",
    "        'best_params': best_params,\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "    }\n",
    "\n",
    "    return results, X_train, X_test, y_train, y_test\n",
    "\n",
    "def main_pipeline(df):\n",
    "    \"\"\"\n",
    "    Main pipeline function to preprocess data, train model, and evaluate performance.\n",
    "    \"\"\"\n",
    "    print(\"Starting classification pipeline...\")\n",
    "\n",
    "    required_columns = [...]  # List of required columns\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "    print(\"\\nPreparing features...\")\n",
    "    X, y, categorical_features, le_y = prepare_features(df)\n",
    "\n",
    "    print(\"\\nTraining and evaluating model...\")\n",
    "    return train_and_evaluate_models_fast(X, y, categorical_features) + (le_y,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catboost agregado y feature engineering mejorado\n",
    "def create_time_features(df):\n",
    "    \"\"\"\n",
    "    Generates time-based features from the dataset.\n",
    "    \n",
    "    Features created:\n",
    "    - Cyclic encoding for the hour of the day using sine and cosine transformations.\n",
    "    - Binary feature indicating whether the day is a weekend.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing a 'Hora_dia' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added time-based features.\n",
    "    \"\"\"\n",
    "    df['hora_sin'] = np.sin(2 * np.pi * df['Hora_dia'] / 24)\n",
    "    df['hora_cos'] = np.cos(2 * np.pi * df['Hora_dia'] / 24)\n",
    "    df['es_fin_semana'] = df['Descripcio_dia_setmana'].isin(['Dissabte', 'Diumenge']).astype(int)\n",
    "    \n",
    "    # Añadir interacción barrio-turno como sugerido\n",
    "    df['barrio_turno'] = df['Nom_barri'] + '_' + df['Descripcio_torn']\n",
    "    \n",
    "    # Crear grupos de barrios según patrones de accidentes similares\n",
    "    grupo_1 = ['Eixample']\n",
    "    grupo_2 = ['Sant_Martí', 'Sarrià-Sant_Gervasi', 'Sants-Montjuïc']\n",
    "    grupo_3 = ['Les_Corts', 'Horta-Guinardó', 'Sant_Andreu', 'Nou_Barris', 'Ciutat_Vella', 'Gràcia']\n",
    "    \n",
    "    # Asignar grupo de barrio\n",
    "    df['grupo_barrio'] = 0\n",
    "    df.loc[df['Nom_barri'].isin(grupo_1), 'grupo_barrio'] = 1\n",
    "    df.loc[df['Nom_barri'].isin(grupo_2), 'grupo_barrio'] = 2\n",
    "    df.loc[df['Nom_barri'].isin(grupo_3), 'grupo_barrio'] = 3\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_distance_to_center(df):\n",
    "    \"\"\"\n",
    "    Calculates the distance of each accident to the geographic center of all accidents.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with UTM coordinates.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added distance to center feature.\n",
    "    \"\"\"\n",
    "    # Calcular el centro geográfico de todos los accidentes\n",
    "    centro_x = df['Coordenada_UTM_X_ED50'].mean()\n",
    "    centro_y = df['Coordenada_UTM_Y_ED50'].mean()\n",
    "    \n",
    "    # Calcular la distancia al centro\n",
    "    df['distancia_centro'] = np.sqrt((df['Coordenada_UTM_X_ED50'] - centro_x)**2 + \n",
    "                                     (df['Coordenada_UTM_Y_ED50'] - centro_y)**2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Prepares features for model training by encoding categorical variables and scaling numerical features.\n",
    "    \n",
    "    Steps:\n",
    "    1. Adds time-based features.\n",
    "    2. Calculates distance to center.\n",
    "    3. Defines categorical and numerical feature lists.\n",
    "    4. Encodes categorical features using Label Encoding.\n",
    "    5. Scales numerical features using StandardScaler.\n",
    "    6. Encodes target variable using Label Encoding.\n",
    "    7. Prints the class distribution.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing raw features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - X_encoded (pd.DataFrame): Processed feature set.\n",
    "            - y (np.array): Encoded target variable.\n",
    "            - categorical_features (list): List of categorical feature names.\n",
    "            - le_y (LabelEncoder): LabelEncoder fitted on the target variable.\n",
    "    \"\"\"\n",
    "    df = create_time_features(df)\n",
    "    df = calculate_distance_to_center(df)\n",
    "\n",
    "    categorical_features = ['Nom_districte', 'Nom_barri', 'Descripcio_dia_setmana', 'Nom_mes', \n",
    "                          'Descripcio_torn', 'barrio_turno', 'grupo_barrio']\n",
    "    \n",
    "    numeric_features = ['Hora_dia', 'Coordenada_UTM_X_ED50', 'Coordenada_UTM_Y_ED50', \n",
    "                       'hora_sin', 'hora_cos', 'es_fin_semana', 'distancia_centro']\n",
    "    \n",
    "    # Eliminamos 'Dia_mes' y 'Mes_any' ya que no presentan patrones significativos\n",
    "\n",
    "    X = df[categorical_features + numeric_features].copy()\n",
    "    y = df['Descripcio_causa_mediata'].copy()\n",
    "\n",
    "    # Encoding categorical features\n",
    "    X_encoded = X.copy()\n",
    "    cat_indices = []\n",
    "    \n",
    "    for i, cat_col in enumerate(categorical_features):\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[cat_col] = le.fit_transform(X[cat_col])\n",
    "        cat_indices.append(i)  # Guardar índices para CatBoost\n",
    "\n",
    "    # Scaling numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_encoded[numeric_features] = scaler.fit_transform(X_encoded[numeric_features])\n",
    "\n",
    "    # Encoding target variable\n",
    "    le_y = LabelEncoder()\n",
    "    y = le_y.fit_transform(y)\n",
    "\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(pd.Series(y).value_counts().sort_index())\n",
    "\n",
    "    return X_encoded, y, categorical_features, cat_indices, le_y\n",
    "\n",
    "def objective(trial, X, y, categorical_features):\n",
    "    \"\"\"\n",
    "    Defines the objective function for hyperparameter optimization using Optuna.\n",
    "    \n",
    "    Steps:\n",
    "    1. Suggests values for LightGBM hyperparameters.\n",
    "    2. Uses StratifiedKFold for cross-validation.\n",
    "    3. Balances the training data using SMOTE.\n",
    "    4. Trains a LightGBM model and evaluates using ROC AUC.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object.\n",
    "        X (pd.DataFrame): Training features.\n",
    "        y (np.array): Target variable.\n",
    "        categorical_features (list): List of categorical feature names.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean ROC AUC score across validation folds.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_class': len(np.unique(y)),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'is_unbalance': True\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(\n",
    "            X_train_res, y_train_res,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=20)]\n",
    "        )\n",
    "\n",
    "        pred = model.predict_proba(X_val)\n",
    "        score = roc_auc_score(y_val, pred, multi_class='ovr')\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def objective_catboost(trial, X, y, cat_indices):\n",
    "    \"\"\"\n",
    "    Defines the objective function for hyperparameter optimization using Optuna for CatBoost.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object.\n",
    "        X (pd.DataFrame): Training features.\n",
    "        y (np.array): Target variable.\n",
    "        cat_indices (list): List of indices for categorical features.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean ROC AUC score across validation folds.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "        'one_hot_max_size': trial.suggest_int('one_hot_max_size', 2, 50),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        'loss_function': 'MultiClass',\n",
    "        'eval_metric': 'WKappa',\n",
    "        'verbose': 0,\n",
    "        'random_seed': 42,\n",
    "        'auto_class_weights': 'Balanced'\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        model = CatBoostClassifier(**params, cat_features=cat_indices)\n",
    "        model.fit(\n",
    "            X_train_res, y_train_res,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        pred = model.predict_proba(X_val)\n",
    "        score = roc_auc_score(y_val, pred, multi_class='ovr')\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def train_and_evaluate_models_fast(X, y, categorical_features, cat_indices):\n",
    "    \"\"\"\n",
    "    Trains and evaluates LightGBM and CatBoost models using Optuna for hyperparameter tuning.\n",
    "    \n",
    "    Steps:\n",
    "    1. Splits data into training and test sets (stratified).\n",
    "    2. Applies SMOTE-Tomek to balance training data.\n",
    "    3. Optimizes hyperparameters with Optuna for both models.\n",
    "    4. Trains final models using best hyperparameters.\n",
    "    5. Evaluates model performance and returns metrics.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature set.\n",
    "        y (np.array): Target variable.\n",
    "        categorical_features (list): List of categorical feature names.\n",
    "        cat_indices (list): List of indices for categorical features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: \n",
    "            - results (dict): Models, classification reports, confusion matrices, feature importances, ROC AUCs.\n",
    "            - X_train, X_test, y_train, y_test (splitted data).\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"\\nApplying SMOTE-Tomek for class balancing...\")\n",
    "    smt = SMOTETomek(random_state=42)\n",
    "    X_train_res, y_train_res = smt.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Optimizar hiperparámetros para LightGBM\n",
    "    print(\"\\nOptimizing LightGBM hyperparameters...\")\n",
    "    lgb_study = optuna.create_study(direction='maximize')\n",
    "    lgb_study.optimize(lambda trial: objective(trial, X_train_res, y_train_res, categorical_features), n_trials=20)\n",
    "\n",
    "    best_lgb_params = lgb_study.best_params\n",
    "    print(\"\\nBest LightGBM hyperparameters:\", best_lgb_params)\n",
    "\n",
    "    # Optimizar hiperparámetros para CatBoost\n",
    "    print(\"\\nOptimizing CatBoost hyperparameters...\")\n",
    "    cb_study = optuna.create_study(direction='maximize')\n",
    "    cb_study.optimize(lambda trial: objective_catboost(trial, X_train_res, y_train_res, cat_indices), n_trials=20)\n",
    "\n",
    "    best_cb_params = cb_study.best_params\n",
    "    print(\"\\nBest CatBoost hyperparameters:\", best_cb_params)\n",
    "\n",
    "    # Entrenar modelo final LightGBM\n",
    "    final_lgb_model = lgb.LGBMClassifier(\n",
    "        **best_lgb_params, random_state=42, verbose=-1, objective='multiclass', num_class=len(np.unique(y))\n",
    "    )\n",
    "\n",
    "    final_lgb_model.fit(X_train_res, y_train_res, eval_set=[(X_test, y_test)], \n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=20)])\n",
    "\n",
    "    lgb_y_pred = final_lgb_model.predict(X_test)\n",
    "    lgb_y_pred_proba = final_lgb_model.predict_proba(X_test)\n",
    "\n",
    "    # Entrenar modelo final CatBoost\n",
    "    final_cb_model = CatBoostClassifier(\n",
    "        **best_cb_params,\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='WKappa',\n",
    "        auto_class_weights='Balanced',\n",
    "        cat_features=cat_indices\n",
    "    )\n",
    "\n",
    "    final_cb_model.fit(\n",
    "        X_train_res, y_train_res,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    cb_y_pred = final_cb_model.predict(X_test)\n",
    "    cb_y_pred_proba = final_cb_model.predict_proba(X_test)\n",
    "\n",
    "    # Resultados para LightGBM\n",
    "    lgb_results = {\n",
    "        'model': final_lgb_model,\n",
    "        'classification_report': classification_report(y_test, lgb_y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_test, lgb_y_pred),\n",
    "        'feature_importance': pd.DataFrame({'feature': X.columns, 'importance': final_lgb_model.feature_importances_})\n",
    "                              .sort_values('importance', ascending=False),\n",
    "        'best_params': best_lgb_params,\n",
    "        'roc_auc': roc_auc_score(y_test, lgb_y_pred_proba, multi_class='ovr')\n",
    "    }\n",
    "\n",
    "    # Resultados para CatBoost\n",
    "    cb_results = {\n",
    "        'model': final_cb_model,\n",
    "        'classification_report': classification_report(y_test, cb_y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_test, cb_y_pred),\n",
    "        'feature_importance': pd.DataFrame({'feature': X.columns, 'importance': final_cb_model.feature_importances_})\n",
    "                              .sort_values('importance', ascending=False),\n",
    "        'best_params': best_cb_params,\n",
    "        'roc_auc': roc_auc_score(y_test, cb_y_pred_proba, multi_class='ovr')\n",
    "    }\n",
    "\n",
    "    # Resultados combinados\n",
    "    results = {\n",
    "        'LightGBM': lgb_results,\n",
    "        'CatBoost': cb_results\n",
    "    }\n",
    "\n",
    "    return results, X_train, X_test, y_train, y_test\n",
    "\n",
    "def analyze_results(results, le_y):\n",
    "    \"\"\"\n",
    "    Analiza y compara los resultados de ambos modelos.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Diccionario con resultados de los modelos.\n",
    "        le_y (LabelEncoder): Codificador de la variable objetivo.\n",
    "    \"\"\"\n",
    "    print(\"\\nComparación de modelos:\")\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Modelo': [],\n",
    "        'ROC AUC': [],\n",
    "        'Precisión': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': []\n",
    "    })\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(f\"ROC AUC: {result['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Extraer métricas del classification_report\n",
    "        cr = classification_report(\n",
    "            np.argmax(result['confusion_matrix'], axis=1),\n",
    "            np.argmax(result['confusion_matrix'], axis=0),\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(result['classification_report'])\n",
    "        \n",
    "        print(\"\\nTop 10 características más importantes:\")\n",
    "        print(result['feature_importance'].head(10))\n",
    "        \n",
    "        # Añadir al dataframe de comparación\n",
    "        precision = cr['weighted avg']['precision'] if 'weighted avg' in cr else 0\n",
    "        recall = cr['weighted avg']['recall'] if 'weighted avg' in cr else 0\n",
    "        f1 = cr['weighted avg']['f1-score'] if 'weighted avg' in cr else 0\n",
    "        \n",
    "        comparison_df = pd.concat([\n",
    "            comparison_df,\n",
    "            pd.DataFrame({\n",
    "                'Modelo': [name],\n",
    "                'ROC AUC': [result['roc_auc']],\n",
    "                'Precisión': [precision],\n",
    "                'Recall': [recall],\n",
    "                'F1 Score': [f1]\n",
    "            })\n",
    "        ])\n",
    "    \n",
    "    print(\"\\nResumen comparativo:\")\n",
    "    print(comparison_df.sort_values('ROC AUC', ascending=False))\n",
    "\n",
    "def main_pipeline(df):\n",
    "    \"\"\"\n",
    "    Main pipeline function to preprocess data, train model, and evaluate performance.\n",
    "    \"\"\"\n",
    "    print(\"Starting classification pipeline...\")\n",
    "\n",
    "    required_columns = ['Nom_districte', 'Nom_barri', 'Descripcio_dia_setmana', \n",
    "                       'Nom_mes', 'Descripcio_torn', 'Hora_dia',\n",
    "                       'Coordenada_UTM_X_ED50', 'Coordenada_UTM_Y_ED50',\n",
    "                       'Descripcio_causa_mediata']\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "    print(\"\\nPreparing features...\")\n",
    "    X, y, categorical_features, cat_indices, le_y = prepare_features(df)\n",
    "\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    results, X_train, X_test, y_train, y_test = train_and_evaluate_models_fast(\n",
    "        X, y, categorical_features, cat_indices\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAnalyzing results...\")\n",
    "    analyze_results(results, le_y)\n",
    "\n",
    "    return results, X_train, X_test, y_train, y_test, le_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 21:09:27,464] Trial 2 finished with value: 0.8621227348248937 and parameters: {'iterations': 262, 'learning_rate': 0.14054109398767478, 'depth': 9, 'l2_leaf_reg': 1.1519997051969635, 'border_count': 55, 'bagging_temperature': 5.079254536653624, 'random_strength': 0.022300603122573124, 'one_hot_max_size': 5, 'min_data_in_leaf': 65}. Best is trial 2 with value: 0.8621227348248937.\n",
      "[I 2025-03-13 21:13:07,217] Trial 3 finished with value: 0.8251407922754368 and parameters: {'iterations': 247, 'learning_rate': 0.2466295829139569, 'depth': 5, 'l2_leaf_reg': 6.296662874921329e-06, 'border_count': 166, 'bagging_temperature': 2.5195066837488165, 'random_strength': 2.6663984786882396e-08, 'one_hot_max_size': 50, 'min_data_in_leaf': 67}. Best is trial 2 with value: 0.8621227348248937.\n",
      "[I 2025-03-13 21:15:17,972] Trial 4 finished with value: 0.6955509496559495 and parameters: {'iterations': 174, 'learning_rate': 0.016653690629284467, 'depth': 5, 'l2_leaf_reg': 5.877948801793553e-08, 'border_count': 96, 'bagging_temperature': 8.48129616029215, 'random_strength': 0.03368282916283866, 'one_hot_max_size': 4, 'min_data_in_leaf': 31}. Best is trial 2 with value: 0.8621227348248937.\n",
      "[I 2025-03-13 21:17:30,513] Trial 5 finished with value: 0.7515517168411051 and parameters: {'iterations': 54, 'learning_rate': 0.13598039695700714, 'depth': 5, 'l2_leaf_reg': 7.2404059761634475e-06, 'border_count': 222, 'bagging_temperature': 7.06903151559633, 'random_strength': 2.3860900613391852e-05, 'one_hot_max_size': 6, 'min_data_in_leaf': 88}. Best is trial 2 with value: 0.8621227348248937.\n",
      "[I 2025-03-13 21:21:04,239] Trial 6 finished with value: 0.7263959337296766 and parameters: {'iterations': 239, 'learning_rate': 0.16023837470509905, 'depth': 6, 'l2_leaf_reg': 0.00514677829968956, 'border_count': 141, 'bagging_temperature': 2.114152050337422, 'random_strength': 7.573543676376577, 'one_hot_max_size': 44, 'min_data_in_leaf': 67}. Best is trial 2 with value: 0.8621227348248937.\n",
      "[I 2025-03-13 21:42:13,942] Trial 7 finished with value: 0.8559350504818314 and parameters: {'iterations': 290, 'learning_rate': 0.024868771620282056, 'depth': 10, 'l2_leaf_reg': 0.027352255786909845, 'border_count': 63, 'bagging_temperature': 8.62535346747644, 'random_strength': 5.021259340766234e-06, 'one_hot_max_size': 50, 'min_data_in_leaf': 73}. Best is trial 2 with value: 0.8621227348248937.\n",
      "[I 2025-03-13 21:46:13,691] Trial 8 finished with value: 0.889264393076556 and parameters: {'iterations': 169, 'learning_rate': 0.11854246557134247, 'depth': 7, 'l2_leaf_reg': 1.909444777798798e-05, 'border_count': 177, 'bagging_temperature': 1.2806631132189805, 'random_strength': 0.1292892873385148, 'one_hot_max_size': 16, 'min_data_in_leaf': 7}. Best is trial 8 with value: 0.889264393076556.\n",
      "[I 2025-03-13 21:49:58,490] Trial 9 finished with value: 0.835712298989514 and parameters: {'iterations': 297, 'learning_rate': 0.1657883078543859, 'depth': 4, 'l2_leaf_reg': 1.1554172713777893e-08, 'border_count': 135, 'bagging_temperature': 0.7198169376516594, 'random_strength': 0.0016472634285094585, 'one_hot_max_size': 18, 'min_data_in_leaf': 70}. Best is trial 8 with value: 0.889264393076556.\n",
      "[I 2025-03-13 22:00:53,597] Trial 10 finished with value: 0.8755752089493616 and parameters: {'iterations': 204, 'learning_rate': 0.21226175700298672, 'depth': 8, 'l2_leaf_reg': 7.896345136938237e-07, 'border_count': 185, 'bagging_temperature': 3.101765165645291, 'random_strength': 9.58177501031161, 'one_hot_max_size': 17, 'min_data_in_leaf': 6}. Best is trial 8 with value: 0.889264393076556.\n",
      "[I 2025-03-13 22:08:15,063] Trial 11 finished with value: 0.8215748898906485 and parameters: {'iterations': 201, 'learning_rate': 0.2061776653701546, 'depth': 8, 'l2_leaf_reg': 4.0716342963536127e-07, 'border_count': 182, 'bagging_temperature': 3.2072931042400095, 'random_strength': 9.263465186565591, 'one_hot_max_size': 19, 'min_data_in_leaf': 7}. Best is trial 8 with value: 0.889264393076556.\n",
      "[I 2025-03-13 22:14:18,164] Trial 12 finished with value: 0.8995861444814978 and parameters: {'iterations': 197, 'learning_rate': 0.08028806990215089, 'depth': 8, 'l2_leaf_reg': 2.6353343270996057e-06, 'border_count': 190, 'bagging_temperature': 0.43465902352597485, 'random_strength': 0.2035619083067257, 'one_hot_max_size': 16, 'min_data_in_leaf': 2}. Best is trial 12 with value: 0.8995861444814978.\n",
      "[I 2025-03-13 22:17:33,224] Trial 13 finished with value: 0.8662261985246321 and parameters: {'iterations': 134, 'learning_rate': 0.07874386400097498, 'depth': 7, 'l2_leaf_reg': 0.0002882963237910776, 'border_count': 109, 'bagging_temperature': 0.23657363948353716, 'random_strength': 0.30823066225926676, 'one_hot_max_size': 25, 'min_data_in_leaf': 27}. Best is trial 12 with value: 0.8995861444814978.\n",
      "[I 2025-03-13 22:30:50,089] Trial 14 finished with value: 0.9029451524684606 and parameters: {'iterations': 203, 'learning_rate': 0.08781497693961872, 'depth': 8, 'l2_leaf_reg': 2.6722362414532245e-05, 'border_count': 205, 'bagging_temperature': 1.3864539302694134, 'random_strength': 4.4470477381563234e-05, 'one_hot_max_size': 13, 'min_data_in_leaf': 22}. Best is trial 14 with value: 0.9029451524684606.\n",
      "[I 2025-03-13 22:51:28,621] Trial 15 finished with value: 0.8595006576441749 and parameters: {'iterations': 213, 'learning_rate': 0.07045793279247116, 'depth': 9, 'l2_leaf_reg': 5.50922766062187e-07, 'border_count': 203, 'bagging_temperature': 4.074570520709562, 'random_strength': 2.012471809263723e-05, 'one_hot_max_size': 11, 'min_data_in_leaf': 27}. Best is trial 14 with value: 0.9029451524684606.\n",
      "[I 2025-03-13 22:55:35,102] Trial 16 finished with value: 0.8765173962138053 and parameters: {'iterations': 109, 'learning_rate': 0.05221140186507394, 'depth': 8, 'l2_leaf_reg': 0.001354315973896834, 'border_count': 249, 'bagging_temperature': 0.04953609038460194, 'random_strength': 4.1205407721860126e-07, 'one_hot_max_size': 27, 'min_data_in_leaf': 43}. Best is trial 14 with value: 0.9029451524684606.\n",
      "[I 2025-03-13 23:37:55,201] Trial 17 finished with value: 0.9041920420907846 and parameters: {'iterations': 219, 'learning_rate': 0.1055970693042763, 'depth': 10, 'l2_leaf_reg': 0.279267314634925, 'border_count': 211, 'bagging_temperature': 1.3872080353322533, 'random_strength': 0.0001429216525130681, 'one_hot_max_size': 11, 'min_data_in_leaf': 19}. Best is trial 17 with value: 0.9041920420907846.\n",
      "[I 2025-03-14 00:06:38,259] Trial 18 finished with value: 0.8755955975962617 and parameters: {'iterations': 236, 'learning_rate': 0.11373543846930248, 'depth': 10, 'l2_leaf_reg': 3.2374209941075263, 'border_count': 221, 'bagging_temperature': 1.762094092589205, 'random_strength': 0.00022618712377465785, 'one_hot_max_size': 11, 'min_data_in_leaf': 47}. Best is trial 17 with value: 0.9041920420907846.\n",
      "[I 2025-03-14 00:25:01,282] Trial 19 finished with value: 0.87529651860016 and parameters: {'iterations': 274, 'learning_rate': 0.19317428805410575, 'depth': 9, 'l2_leaf_reg': 0.2880063848034791, 'border_count': 159, 'bagging_temperature': 4.5979127327582265, 'random_strength': 1.4346437677278762e-06, 'one_hot_max_size': 11, 'min_data_in_leaf': 19}. Best is trial 17 with value: 0.9041920420907846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best CatBoost hyperparameters: {'iterations': 219, 'learning_rate': 0.1055970693042763, 'depth': 10, 'l2_leaf_reg': 0.279267314634925, 'border_count': 211, 'bagging_temperature': 1.3872080353322533, 'random_strength': 0.0001429216525130681, 'one_hot_max_size': 11, 'min_data_in_leaf': 19}\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's multi_logloss: 2.54676\n",
      "\n",
      "Analyzing results...\n",
      "\n",
      "Comparación de modelos:\n",
      "\n",
      "=== LightGBM ===\n",
      "ROC AUC: 0.5560\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.05      0.06       218\n",
      "           1       0.04      0.03      0.04        59\n",
      "           2       0.14      0.11      0.12       133\n",
      "           3       0.10      0.09      0.10        76\n",
      "           4       0.10      0.09      0.09        92\n",
      "           5       0.11      0.09      0.10       128\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.15      0.17      0.16       161\n",
      "           9       0.26      0.37      0.31       412\n",
      "          10       0.05      0.06      0.05        35\n",
      "          11       0.00      0.00      0.00        48\n",
      "          12       0.00      0.00      0.00        14\n",
      "          13       0.11      0.06      0.08       149\n",
      "          14       0.19      0.17      0.18       199\n",
      "          15       0.09      0.10      0.09        48\n",
      "\n",
      "    accuracy                           0.16      1780\n",
      "   macro avg       0.09      0.09      0.09      1780\n",
      "weighted avg       0.15      0.16      0.15      1780\n",
      "\n",
      "\n",
      "Top 10 características más importantes:\n",
      "                   feature  importance\n",
      "8    Coordenada_UTM_X_ED50        2632\n",
      "9    Coordenada_UTM_Y_ED50        2469\n",
      "13        distancia_centro        2191\n",
      "5             barrio_turno        1815\n",
      "3                  Nom_mes        1808\n",
      "11                hora_cos        1585\n",
      "10                hora_sin        1507\n",
      "2   Descripcio_dia_setmana        1217\n",
      "7                 Hora_dia        1197\n",
      "1                Nom_barri        1150\n",
      "\n",
      "=== CatBoost ===\n",
      "ROC AUC: 0.5462\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.08      0.10       218\n",
      "           1       0.05      0.08      0.06        59\n",
      "           2       0.16      0.09      0.12       133\n",
      "           3       0.06      0.08      0.07        76\n",
      "           4       0.06      0.09      0.07        92\n",
      "           5       0.13      0.12      0.12       128\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.10      0.11      0.10       161\n",
      "           9       0.36      0.13      0.19       412\n",
      "          10       0.04      0.17      0.06        35\n",
      "          11       0.02      0.02      0.02        48\n",
      "          12       0.03      0.29      0.06        14\n",
      "          13       0.15      0.11      0.13       149\n",
      "          14       0.23      0.16      0.19       199\n",
      "          15       0.09      0.23      0.13        48\n",
      "\n",
      "    accuracy                           0.11      1780\n",
      "   macro avg       0.10      0.11      0.09      1780\n",
      "weighted avg       0.18      0.11      0.13      1780\n",
      "\n",
      "\n",
      "Top 10 características más importantes:\n",
      "                  feature  importance\n",
      "3                 Nom_mes   22.583047\n",
      "1               Nom_barri   11.717997\n",
      "8   Coordenada_UTM_X_ED50   11.057713\n",
      "9   Coordenada_UTM_Y_ED50   10.931358\n",
      "13       distancia_centro   10.517246\n",
      "11               hora_cos    9.701441\n",
      "12          es_fin_semana    7.342271\n",
      "4         Descripcio_torn    4.624683\n",
      "10               hora_sin    4.286817\n",
      "7                Hora_dia    4.226539\n",
      "\n",
      "Resumen comparativo:\n",
      "     Modelo   ROC AUC  Precisión  Recall  F1 Score\n",
      "0  LightGBM  0.555960   0.758333  0.8125  0.784483\n",
      "0  CatBoost  0.546235   0.141667  0.1875  0.112745\n"
     ]
    }
   ],
   "source": [
    "results, X_train, X_test, y_train, y_test, le_y = main_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The traffic accident prediction models didn't perform as well as I hoped. The LightGBM model reached an accuracy of 16% and ROC-AUC of 0.556, while CatBoost performed slightly worse with 11% accuracy and ROC-AUC of 0.546. These numbers are just a bit better than random guessing for this multi-class problem.\n",
    "I tried to tackle several challenges throughout this project:\n",
    "\n",
    "Time data: I solved this by creating special cyclic features (hora_sin and hora_cos) to represent the time of day properly, and added a weekend flag to capture weekly patterns.\n",
    "Imbalanced classes: I addressed this using SMOTE-Tomek to generate balanced training data, but the models still struggled with rare accident causes (especially classes 6 and 7).\n",
    "Finding the right parameters: Used Optuna to automatically search for the best settings for both models, running multiple trials to find optimal configurations.\n",
    "\n",
    "Despite these efforts, the models had a hard time distinguishing between the 16 different accident causes. The classification reports show that precision, recall, and F1 scores were consistently low across most classes.\n",
    "The most interesting finding was in the feature importance rankings. Location seems to matter a lot - UTM coordinates and distance to city center were top predictors for LightGBM. Time patterns were also important, with month, time of day, and weekend status ranking high for both models.\n",
    "These results show just how complex traffic accident prediction really is. While we can't reliably predict the exact cause of an accident yet, the patterns we found in location and time could still help city planners and traffic authorities know where and when to focus safety efforts.\n",
    "For future work, we might try:\n",
    "\n",
    "Grouping similar accident causes together to create broader categories\n",
    "Adding more relevant features like weather conditions or traffic density\n",
    "Testing different machine learning approaches beyond tree-based models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
